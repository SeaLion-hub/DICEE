import logging
import re
import base64
import os
from datetime import datetime
from urllib.parse import urljoin
import httpx
from bs4 import BeautifulSoup, NavigableString
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.crawler_config import CRAWLER_CONFIG
from app.models.notice import Notice
from app.models.college import College

logger = logging.getLogger(__name__)

class YonseiEngineeringCrawler:
    def __init__(self, session: AsyncSession):
        self.session = session
        self.config = CRAWLER_CONFIG["engineering"]
        self.college_code = "engineering"

    # --- [app2.py Î°úÏßÅ 1] HTML Ìëú Î∞è ÌÖçÏä§Ìä∏ Íµ¨Ï°∞ Î≥¥Ï°¥ ---
    def process_table_html(self, table_tag):
        for tag in table_tag(['script', 'style', 'noscript', 'iframe']):
            tag.decompose()
        if not table_tag.get('border'):
            table_tag['border'] = "1"
        return str(table_tag)

    def get_text_structurally(self, element):
        text_content = ""
        if isinstance(element, NavigableString):
            return str(element)
        if element.name == 'table':
            return self.process_table_html(element)
        
        for child in element.children:
            if child.name in ['script', 'style', 'noscript']: continue
            if child.name == 'br':
                text_content += '\n'
                continue
            child_text = self.get_text_structurally(child)
            block_tags = ['div', 'p', 'li', 'tr', 'h1', 'h2', 'h3', 'option', 'dd', 'dt']
            if child.name in block_tags:
                if child_text.strip() or "<table" in child_text:
                    text_content += "\n" + child_text.strip() + "\n"
            else:
                text_content += child_text
        return text_content

    def finalize_text(self, text):
        text = re.sub(r'\n\s*\n+', '\n\n', text)
        return text.strip()

    # --- Ïã§Ìñâ ÏßÑÏûÖÏ†ê ---
    async def run(self):
        logger.info(f"[{self.college_code}] ÌÅ¨Î°§ÎßÅ ÏãúÏûë (app2.py Î°úÏßÅ Ï†ÅÏö©)...")

        # 1. Îã®Í≥ºÎåÄ Ï†ïÎ≥¥ Ï°∞Ìöå
        stmt = select(College).where(College.external_id == self.college_code)
        result = await self.session.execute(stmt)
        college = result.scalar_one_or_none()

        if not college:
            logger.error(f"College not found: {self.college_code}")
            return

        college_db_id = college.id 

        # 2. Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.get(self.config["url"])
                response.raise_for_status()
            except Exception as e:
                logger.error(f"Î™©Î°ù ÌéòÏù¥ÏßÄ Ï†ëÏÜç Ïã§Ìå®: {e}")
                return

            soup = BeautifulSoup(response.text, 'html.parser')
            rows = soup.select(self.config["selectors"]["row"])

            for row in rows:
                cols = row.find_all('td')
                if not cols: continue

                # Î≤àÌò∏ ÌôïÏù∏ (Í≥µÏßÄÏÇ¨Ìï≠ ÌïÑÌÑ∞ÎßÅ)
                num_text = cols[0].get_text(strip=True)
                if not num_text.isdigit(): continue
                
                external_id = num_text
                
                # Ï§ëÎ≥µ ÌôïÏù∏ (Ïù¥ÎØ∏ ÏûàÏúºÎ©¥ Ìå®Ïä§)
                stmt = select(Notice).where(
                    Notice.college_id == college_db_id, 
                    Notice.external_id == external_id
                )
                res = await self.session.execute(stmt)
                if res.scalar_one_or_none():
                    continue

                link_tag = row.find('a')
                if not link_tag or not link_tag.get('href'): continue
                
                detail_url = urljoin(self.config["url"], link_tag['href'])

                # ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄ ÌååÏã± (app2.py Î°úÏßÅ)
                await self.parse_detail_and_save(client, detail_url, college_db_id, external_id)

    # --- [app2.py Î°úÏßÅ 2] ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄ ÌååÏã± (Ï†ïÎ∞Ä ÌÉÄÍ≤©) ---
    async def parse_detail_and_save(self, client, url, college_id, external_id):
        try:
            res = await client.get(url)
            soup = BeautifulSoup(res.text, 'html.parser')

            # 1. Ï†úÎ™© Ï∂îÏ∂ú
            title = "Ï†úÎ™© ÏóÜÏùå"
            title_label = soup.find(string=lambda t: t and "Ï†úÎ™©" in t)
            if title_label:
                title_container = title_label.find_parent(['dt', 'th', 'td'])
                if title_container:
                    title_elem = title_container.find_next_sibling(['dd', 'td'])
                    if title_elem:
                        title = self.get_text_structurally(title_elem).strip()
            
            if title == "Ï†úÎ™© ÏóÜÏùå":
                h3 = soup.find('h3')
                if h3: title = self.get_text_structurally(h3).strip()

            # 2. ÎÇ†Ïßú Ï∂îÏ∂ú (app2.py Ï†ïÍ∑úÏãù)
            pub_date = None
            date_match = re.search(r'\d{4}[.-]\d{2}[.-]\d{2}', soup.get_text())
            if date_match:
                try:
                    date_str = date_match.group().replace('.', '-')
                    pub_date = datetime.strptime(date_str, "%Y-%m-%d")
                except: pass

            # 3. Î≥∏Î¨∏ Ï∂îÏ∂ú (Garbage Ï†úÍ±∞ Î°úÏßÅ Ìè¨Ìï®)
            content_text = ""
            main_container = None
            anchor_text = soup.find(string=lambda t: t and "Í≤åÏãúÍ∏Ä ÎÇ¥Ïö©" in t)
            
            if anchor_text:
                start_tag = anchor_text.find_parent(['dt', 'th', 'td'])
                if start_tag:
                    target_body = start_tag.find_next_sibling(['dd', 'td'])
                    if target_body:
                        main_container = target_body
                        
                        # [app2.py] Ïì∞Î†àÍ∏∞ ÏöîÏÜå Ï†úÍ±∞
                        garbage_selectors = ['.btn_area', '.btn-wrap', '#bo_v_share', 'ul.btn_bo_user', 'div.btn_confirm']
                        for selector in garbage_selectors:
                            for tag in main_container.select(selector):
                                tag.decompose()
                        
                        raw_text = self.get_text_structurally(main_container)
                        
                        # [app2.py] Î∂àÌïÑÏöî ÌÇ§ÏõåÎìú Ïù¥ÌõÑ Ï†àÏÇ≠
                        stop_keywords = ["Í¥ÄÎ¶¨Ïûê ifÎ¨∏", "ÎãµÎ≥ÄÍ∏Ä Î≤ÑÌäº", "Î™©Î°ù List Î≤ÑÌäº", "Îì±Î°ù Î≤ÑÌäº"]
                        for keyword in stop_keywords:
                            if keyword in raw_text:
                                raw_text = raw_text.split(keyword)[0]
                                
                        content_text = self.finalize_text(raw_text)

            if not content_text:
                content_text = "(Î≥∏Î¨∏ ÎÇ¥Ïö©ÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§)"

            # 4. Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂ú (Base64 + URL Î™®Îëê ÏßÄÏõê)
            images_data = []
            poster_url = None
            
            if main_container:
                img_tags = main_container.find_all('img')
                for idx, img in enumerate(img_tags):
                    src = img.get('src', '')
                    if not src: continue
                    
                    # Base64 Ï≤òÎ¶¨
                    if src.startswith('data:image'):
                        # DBÏóêÎäî Ïö©Îüâ Î¨∏Ï†úÎ°ú base64 ÏßÅÏ†ë Ï†ÄÏû•ÏùÄ ÎπÑÏ∂îÏ≤úÌïòÏßÄÎßå, 
                        # app2.py Î°úÏßÅÏùÑ Îî∞Î•¥Í∏∞ ÏúÑÌï¥ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌòïÌÉúÎ°ú Ï†ÄÏû•ÌïòÍ±∞ÎÇò Ïä§ÌÇµÌï©ÎãàÎã§.
                        # Ïó¨Í∏∞ÏÑúÎäî 'url'Ïù¥ ÏóÜÏúºÎØÄÎ°ú Ï†úÏô∏ÌïòÍ±∞ÎÇò, ÌïÑÏöîÏãú Î≥ÑÎèÑ Ï≤òÎ¶¨Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.
                        # ÌòÑÏû¨Îäî URL Í∏∞Î∞ò ÌÅ¨Î°§Îü¨Ïù¥ÎØÄÎ°ú base64Îäî Í±¥ÎÑàÎõ∞Í±∞ÎÇò ÌÉúÍ∑∏Îßå ÎÇ®ÍπÅÎãàÎã§.
                        continue 
                    else:
                        # URL Ï≤òÎ¶¨
                        if any(x in src for x in ['icon', 'btn', 'button', 'search', 'blank']): continue
                        
                        full_url = urljoin(url, src)
                        
                        # Ï§ëÎ≥µ Ï†úÍ±∞
                        if any(d.get('url') == full_url for d in images_data): continue
                        
                        images_data.append({
                            "type": "url",
                            "url": full_url
                        })

            if images_data:
                poster_url = images_data[0]['url']

            # 5. Ï≤®Î∂ÄÌååÏùº Ï∂îÏ∂ú ("Ï≤®Î∂Ä" ÌÖçÏä§Ìä∏ Í∏∞Ï§Ä ÌÉêÏÉâ)
            attachments = []
            attach_labels = soup.find_all(string=re.compile("Ï≤®Î∂Ä"))
            for label in attach_labels:
                parent_row = label.find_parent(['tr', 'li', 'div', 'dl', 'dt', 'dd'])
                if parent_row and parent_row.name == 'dt':
                    parent_row = parent_row.find_next_sibling('dd')
                if parent_row:
                    links = parent_row.find_all('a')
                    for link in links:
                        file_name = link.get_text(strip=True)
                        href = link.get('href', '')
                        if href and not href.startswith('#') and 'javascript' not in href:
                             full_href = urljoin(url, href)
                             # Ï§ëÎ≥µ Î∞©ÏßÄ
                             if not any(a['url'] == full_href for a in attachments):
                                 attachments.append({"name": file_name, "url": full_href})

            # DB Ï†ÄÏû•
            new_notice = Notice(
                college_id=college_id,
                external_id=external_id,
                title=title,
                raw_html=content_text,
                url=url,
                published_at=pub_date,
                poster_image_url=poster_url,
                images=images_data,      # JSONB Ï†ÄÏû•
                attachments=attachments, # JSONB Ï†ÄÏû•
            )
            self.session.add(new_notice)
            await self.session.commit()
            
            # Î°úÍ∑∏ Ï∂úÎ†• (app2.py Ïä§ÌÉÄÏùº)
            log_img_cnt = len(images_data)
            log_file_cnt = len(attachments)
            logger.info(f"‚úÖ Saved: [{external_id}] {title[:20]}... (üìÖ{pub_date} | üñºÔ∏è{log_img_cnt} | üìé{log_file_cnt})")

        except Exception as e:
            logger.error(f"Failed to parse {url}: {e}")
            await self.session.rollback()